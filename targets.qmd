# Build automation

We are finally ready to actually build a pipeline. For this, we are going to be
using a package called `{targets}` which is a so-called build automation tool.

Without a build automation tool, a pipeline is nothing but a series of scripts
that get called one after the other, or perhaps the pipeline is only one
very long script that does the required operations successfully.

There are several problems with this approach.

## Introduction

Scripts can, and will, be executed out of order. You can mitigate the problems
this can make by using pure functions, but even then, you risk messing things up
when you do this. But why would you run your script out of order? Well, usually
because you changed a function, and only want to re-execute the parts of the
pipeline that are impacted by that change. But this supposes that you can know,
in your head, which part of the script was impacted and which was not. And this
can be quite difficult to figure out, especially when the pipeline is huge.

Another issue, but this one is perhaps more subjective, is that pipelines
written as scripts are usually quite difficult to read and understand. To
mitigate this, what you'd typically do is write a lot of comments. But here
again you face the problem of needing to maintain these comments, and once the
comments and the code are out of synch... the problems start.

Running the different parts of the pipeline in parallel is also very complicated
if your pipeline is defined as script. You would need to break the script into
independent parts (and make really sure that these parts are independent) and
execute them in parallel, perhaps using a separate R session for each script.
The good news is that if you followed the advice from this book is that you have
been using functional programming and so your pipeline is a series of function
calls, which is "easy" to break up and run in parallel.

But by now you should now that software engineers also faced similar problems
when they needed to build their software, and you should also know that they
likely came up with something to alleviate these issues. Enter build automation
tools.

When using a build automation tool, what you end up doing is writing down a
recipe, that will not be very different than a standard script in your favourite
programming language, that defines how the source code should be "cooked" into
the software (or in our case, a report, a cleaned dataset or any data product).

The build automation tool can then figure out the following things:

- any change in any of the code. Only the outputs that are affected by the changes you did will be re-computed (and their dependencies as well);
- any change in any of the tracked files. For example, if a file gets updated daily, you can track this file and the build automation tool will only execute the parts of the pipeline affected by this update;
- which parts of the pipeline can safely run in parallel (with the option to thus run the pipeline on multiple CPU cores).

Just like many of the other tools that we have encountered in this book, what
build automation tools do is allow you to not have to rely on your brain. You
*write down* the recipe once, and then you can focus again, on just the code of
your actual project. You shouldn't have to think about the pipeline itself, nor
think about to best run it. Let your computer figure that out for you, it's much
better at such tasks than you.

## {targets} quick-start

First thing's first: to know everything about the `{targets}` package, you
should read the excellent [`{targets}`
manual](https://books.ropensci.org/targets/)^[https://is.gd/VS6vSs]. 
Everything's in there. So what I'm going to do is really to just give you 
a very quick intro to what I think are really the main points that you should
know about.

Let's start by a "hello-world" type pipeline. Create a file called `_targets.R`
in a new folder called something like `targets_intro/`.

## Handling files

## Running the pipeline in parallel

## {targets} and {renv}

*Why build automation: removes cognitive load, is a form of documentation in and of itself, as
Miles said*

*It is possible to communicate a great deal of domain knowledge in code, such that it is illuminating beyond the mere mechanical number crunching. To do this well the author needs to make use of certain styles and structures that produce code that has layers of domain specific abstraction a reader can traverse up and down as they build their understanding of the project. Functional programming style, coupled with a dependency graph as per {targets} are useful tools in this regard.*
