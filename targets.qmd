# Build automation

We are finally ready to actually build a pipeline. For this, we are going to be
using a package called `{targets}` which is a so-called build automation tool.

If you go back to the reproducibility iceberg, you will see that we are quite
low now.

Without a build automation tool, a pipeline is nothing but a series of scripts
that get called one after the other, or perhaps the pipeline is only one very
long script that does the required operations successfully.

There are several problems with this approach, so let’s see how build automation
can help us.

## Introduction

Script-based workflows are problematic for several reasons. The first is that
scripts can, and will, be executed out of order. You can mitigate the problems
this can make by using pure functions, but you still need to make sure to not
run the scripts out of order. But what does that actually mean? Well, suppose
that you changed a function, and only want to re-execute the parts of the
pipeline that are impacted by that change. But this supposes that you can know,
in your head, which part of the script was impacted and which was not. And this
can be quite difficult to figure out, especially when the pipeline is huge. So
you will run certain parts of the script, and not others, in the hope that you 
don’t need to re-run everything.

Another issue, but this one is perhaps more subjective, is that pipelines
written as scripts are usually quite difficult to read and understand. To
mitigate this, what you'd typically do is write a lot of comments. But here
again you face the problem of needing to maintain these comments, and once the
comments and the code are out of synch... the problems start (or rather, they 
continue).

Running the different parts of the pipeline in parallel is also very complicated
if your pipeline is defined as script. You would need to break the script into
independent parts (and make really sure that these parts are independent) and
execute them in parallel, perhaps using a separate R session for each script.
The good news is that if you followed the advice from this book you have
been using functional programming and so your pipeline is a series of function
calls, which is "easy" to break up and run in parallel.

But by now you should know that software engineers also faced similar problems
when they needed to build their software, and you should also suspect that they
likely came up with something to alleviate these issues. Enter build automation
tools.

When using a build automation tool, what you end up doing is writing down a
recipe, that will not be very different than a standard script in your favourite
programming language, that defines how the source code should be "cooked" into
the software (or in our case, a report, a cleaned dataset or any data product).

The build automation tool can then figure out the following things:

- any change in any of the code. Only the outputs that are affected by the changes you did will be re-computed (and their dependencies as well);
- any change in any of the tracked files. For example, if a file gets updated daily, you can track this file and the build automation tool will only execute the parts of the pipeline affected by this update;
- which parts of the pipeline can safely run in parallel (with the option to thus run the pipeline on multiple CPU cores).

Just like many of the other tools that we have encountered in this book, what
build automation tools do is allow you to not have to rely on your brain. You
*write down* the recipe once, and then you can focus again on just the code of
your actual project. You shouldn't have to think about the pipeline itself, nor
think about how to best run it. Let your computer figure that out for you, it's
much better at such tasks than you.

## {targets} quick-start

First thing's first: to know everything about the `{targets}` package, you
should read the excellent [`{targets}`
manual](https://books.ropensci.org/targets/)^[https://is.gd/VS6vSs].
Everything's in there. So what I'm going to do is really just give you a very
quick intro to what I think are really the main points you should know about to
get started.

Let's start by a "hello-world" type pipeline. Create a new folder called
something like `targets_intro/`, and start a fresh R session in it. For now, let’s
ignore `{renv}`. We will see how `{renv}` works together with `{targets}` to provide
a reproducible pipeline later. In that fresh session inside the `targets_intro/`
run the following line:

```{r, eval = FALSE}
targets::tar_script()
```

this will create a template `_targets.R` file in that directory. This is the
file in which we will define our pipeline. Open in it in your favourite editor.
A `_targets.R** pipeline is roughly divided into three parts:

- first is where packages are loaded and helper functions defined;
- second is where pipeline-specific options are defined;
- third is the pipeline itself, defined as a series of *targets*.

Let’s go through all these parts one by one.

### _targets.R’s anatomy

The first part of the pipeline is where packages get loaded as well as helper
functions. In the template, the very first line is a `library(targets)` call
followed by a function definition. There are two important things here that you
need to understand.

If your pipeline needs, say, the `{dplyr}` package to run, you could write
`library(dplyr)` right after the `library(targets)` call. However, it is best to
actually do as in the template, and load the packages using
`tar_option_set(packages = "dplyr")`. This is because if you execute the
pipeline in a cluster, you need to make sure that all the packages are available
to all the workers. If you load the packages at the top of the `_targets.R`
script, the packages will be available for the session that called
`library(...)`, but not any worker sessions spawned for parallel execution.
  
So, the idea is that at the very top of your script, you only load the
`{targets}` library and other packages that are required for running the
pipeline itself (as we shall see in coming sections). But packages that are
required by functions that are running inside the pipeline should ideally be
loaded as in the template. Another way of saying this: at the top of the script,
think "pipeline infrastructure" packages (`{targets}` and some others), but
inside `tar_option_set()` think "packages for functions that run inside the
pipeline" packages.

Part two is where you set some global options for the pipeline. As discussed
previously, this is where you should load packages that are required by the
functions that are called inside the pipeline. I won’t list all the options
here, because I would simply be repeating what’s in the
[documentation](https://docs.ropensci.org/targets/reference/tar_option_set.html)^[https://is.gd/lm4QoO].
This second part is also where you can define some functions that you might need
for running the pipeline. For example, you might need to define a function to
load and clean some data: this is where you would do so. We have developed a
package, so we might not need this. But sometimes your analysis doesn’t require
you to write any custom functions, or maybe just a few, and perhaps you don’t
see the benefit of building a package just for one or two functions. So instead,
you have two other options: you either define them directly inside the
`_targets.R` script, like in the template, or you create a `functions/` folder
next to the `_targets.R` script, and put your functions there. It’s up to you,
but I prefer this second option.

Finally, comes the pipeline itself. Let’s take a closer look at it:

```{r, eval = F}
list(
  tar_target(
    data,
    data.frame(x = sample.int(100),
               y = sample.int(100))
  ),

  tar_target(
    summary,
    summ(data)) # Call your custom functions as needed.
)
```

The pipeline is nothing but a list (told you lists where a very important
object) of *targets*. A target is defined using the `targets::tar_target()`
function and has at least two inputs: the first is the name of the target
(without quotes) and the second is the function that generates the target. So a
target defined as `tar_target(y, f(x))` can be understood as `y <- f(x)`. The next
target can use the output of the previous target as an input, so you could have
something like `tar_target(z, f(y))` (just like in the template). 

## A pipeline is a composition of pure functions

This pipeline can immediately be run using the `targets::tar_make()` command in
a console. Doing so shows you the following output:

```{r, eval = F}
targets::tar_make()
```

```
• start target data
• built target data [0.82 seconds]
• start target summary
• built target summary [0.02 seconds]
• end pipeline [1.71 seconds]
```

The pipeline is done running! So, now what? This pipeline simply built some
summary statistics, but where are they? Typing `summary` in the console to try
to inspect this output results in the following:

```{r, eval = F}
summary
```

```
function (object, ...) 
UseMethod("summary")
<bytecode: 0x000001f1a5436d78>
<environment: namespace:base>
```

What is going on?

First, you need to remember our chapter on functional programming. We want our
pipeline to be a sequence of pure functions. This means that our pipeline
running successfully should not depend on anything in the global environment
(apart from loading the packages in the first part of the script, and the
options set with `tar_option_set()` for the others) and it should not change
anything outside of its scope. This means that the pipeline should not change
anything in the global environment either. This is exactly how a `{targets}`
pipeline operates. A pipeline defined using `{targets}` will be pure and so the
output of the pipeline will not be saved in the global environment. Now, strictly
speaking, the pipeline is not exactly pure. Check the folder that contains the
`_targets.R` script. There should now be a `_targets/` folder in there as well.
If you go inside that folder, and then open the `objects/` folder, you should
see two objects, `data` and `summary`. These are the outputs of our pipeline.

So each target that is defined inside the pipeline gets saved there in the
`.rds` format. This is an R-specific format that can be used to save *any* type
of object. It doesn’t matter what it is: a simple data frame, a fitted model, a
ggplot, whatever, any object can be saved into this format using the `saveRDS()`
function, and can be read back into another R session using `readRDS()`.
`{targets}` makes use of these two functions to save every target. Keep this in
mind if you use Git to version the code of your pipeline (which you are doing of
course), and add the `targets/` folder to the `.gitignore` (unless you really
want to also version it, but it shouldn’t be necessary).

Ok, so back to `summary`. What happened before when we typed `summary` in the
console? Well, now you know why the result didn’t appear: this is because the
target called `summary` gets saved in that folder, but not in the global
environment. So why did we see something anyways, which was not looking like a
data summary at all?

This is because R has a `summary()` function. So when you write the function’s name
in the console without `()` you see the function’s source code. Let’s try again:

```{r, eval = F}
summary
```


```{r, eval = F}
function (object, ...) 
UseMethod("summary")
<bytecode: 0x000001f1a5436d78>
<environment: namespace:base>
```

Ok, so here, strictly speaking, you don’t see the source code from `summary()`.
This is because `summary()` is a generic function, that simply picks the right
`summary()` depending on what type of object you pass to it. Try with
`summary.data.frame()` and yous should see the source code. So when you type
`summary` after running the pipeline, because the pipeline is pure and nothing
gets saved to the global environment, you see `summary`’s source code and not
the target that got computed by the pipeline.

So that was a long explanation, but I think that it was worth the effort. It is
quite important to not forget that a targets pipeline is the composition of many
pure functions, but since we need to be able to access the outputs produced by
the pipeline, they also get saved inside a folder. To retrieve the outputs you
can use `targets::tar_read()` or `targets::tar_load()`. The difference is that
`tar_read()` simply reads the output and shows it in the console (just like when
you type `mtcars` in a console for example) but `tar_load()` reads and saves the
object into the global environment (just like doing something like `x <- mtcars`
in an R console). So to retrieve our `summary` object let’s use
`tar_load(summary)`:

```{r, eval = F}
tar_load(summary)
```

Now, typing `summary` shows the computed output (now the function
`base::summary()` is masked):

```{r, eval = F}
summary
```

```{r, eval = F}
    mean_x
1 500000.5
```

It is possible to load all the outputs using `targets::tar_load_everything()` so
that you don’t need to load each output one by one.

Before continuing with more `{targets}` features, I want to really stress the
fact that the pipeline is the composition of pure functions. So functions that
only have a side-effect will be difficult to handle. For example, plotting in
base R consists in a series of calls to functions with side-effects. If you open
an R console and type `plot(mtcars)`, you will see a plot. But the function
`plot()` does not create any output. It just prints a picture on your screen,
which is a side-effect. To convince yourself that `plot()` does not create any
output and only has a side-effect, try to save the output of `plot()` in a
variable:

```{r, eval = F}
a <- plot(mtcars)
```

doing this will show the plot, but then if you call `a`, the plot will not
appear, and instead you will get NULL:

```{r, eval = F}
a
```

```{r, eval = F}
NULL
```

This is also why saving plots in R is awkward, it’s because there’s no object to
actually save!

So because `plot()` is not a pure function, if you try to use it in a
`{targets}` pipeline, you will get `NULL` as well when loading the target. To
see this, change the list of targets like this:

```{r, eval = F}
list(
  tar_target(
    data,
    data.frame(x = sample.int(100),
               y = sample.int(100))
  ),

  tar_target(
    summary,
    summ(data)
  ),

  tar_target(
    data_plot,
    plot(data)
  )
)
```

I’ve simply added a new target using `tar_target()`. Run the pipeline again
using `tar_make()` and then type `tar_load(data_plot)` to load the `data_plot`
target. But typing `data_plot` only shows `NULL` and not the plot!

There are several workarounds for this. The first is to use `ggplot()` instead.
This is because the output of `ggplot()` is an object of type `ggplot`. You can
do something like `a <- ggplot()` and then type `a` to see an empty canvas.
Doing `str(a)` also shows an output (the structure of the object a of class
`ggplot`).

The second workaround is to save the plot to disk. For this, you need to write a
new function, for example:

```{r, eval = F}
save_plot <- function(filename, ...){

  png(filename = filename)
  plot(...)
  dev.off()

}
```

If you put this in the `_targets.R` script, before defining the list of
`tar_target` objects, you could use this instead of `plot()` in the last target:

```{r, eval = F}
summ <- function(dataset) {
  summarize(dataset, mean_x = mean(x))
}

save_plot <- function(filename, ...){
  png(filename = filename)
  plot(...)
  dev.off()

  filename
}

# Set target-specific options such as packages.
tar_option_set(packages = "dplyr")

# End this file with a list of target objects.
list(
  tar_target(
    data,
    data.frame(x = sample.int(100),
               y = sample.int(100))
  ),

  tar_target(
    summary,
    summ(data)
  ),

  tar_target(
    data_plot,
    save_plot(filename = "my_plot.png",
              data),
              format = "file")
)
```

After running this pipeline you should see a file called `my_plot.png` in the
folder of your pipeline. If you type `tar_load(data_plot)`, and then `data_plot`
you will see that this target returns the `filename` argument of `save_plot()`.
This is because a target needs to return something, and in the case of functions
that save a file to disk returning the path is recommended. This is because if I
then need to use this file in another target, I could do `tar_target(x,
f(data_plot))`. Because the `data_plot` target returns a path, I can write `f()`
in such a way that it knows how to handle this path. If instead I write
`tar_target(x, f("path/to/my_plot.png"))`, then `{targets}` would have no way of
knowing that the target `x` depends on the target `data_plot`. The dependency
between these two targets would break. Hence why the first option is preferable.

Finally, you will have noticed that the last target also has the option `format =
"file"`. This will be topic of the next section.

## Handling files

In this section, we will learn how `{targets}` handles files. First, run the
following lines in the folder that contains the `_targets.R` script that we’ve
been using up until now:

```{r, eval = F}
data(mtcars)

write.csv(mtcars,
          "mtcars.csv",
          row.names = F)
```

This will create the file `"mtcars.csv"` in that folder. We are going to use
this in our pipeline.

Write the pipeline like this:

```{r, eval = F}
list(
  tar_target(
    data_mtcars,
    read.csv("mtcars.csv")
  ),

  tar_target(
    summary_mtcars,
    summary(data_mtcars)
  ),

  tar_target(plot_mtcars,
             save_plot(
               filename = "mtcars_plot.png",
               data_mtcars),
             format = "file")
)
```

You can now run the pipeline and will get a plot at the end. The problem
however, is that the input file `"mtcars.csv"` is not being tracked for changes.
Try to change the file, for example by running this line in the console:

```{r, eval = F}
write.csv(head(mtcars), "mtcars.csv", row.names = F)
```

If you try to run the pipeline again, our changes to the data are ignored:

```
✔ skip target data_mtcars
✔ skip target plot_mtcars
✔ skip target summary_mtcars
✔ skip pipeline [0.1 seconds]
```

As you can see, because `{targets}` is not tracking the changes in the `mtcars.csv` file,
from the its point of view nothing changed. And thus the pipeline gets skipped because 
according to `{targets}`, it is up-to-date.

Let’s change the csv back:

```{r, eval = F}
write.csv(mtcars, "mtcars.csv", row.names = F)
```

and change the first target such that the file gets tracked. Remember that
targets need to be pure functions and return something. So we are going to
change the first target to simply return the path to the file, and use the
`format = "file"` option in `tar_target()`:

```{r, eval = F}
path_data <- function(path){
  path
}

list(
  tar_target(
    path_data_mtcars,
    path_data("mtcars.csv"),
    format = "file"
  ),
  tar_target(data_mtcars,
             read.csv(path_data_mtcars)
             ),
  tar_target(
    summary_mtcars,
    summary(data_mtcars)
  ),
  tar_target(plot_mtcars,
             save_plot(filename = "mtcars_plot.png",
                       data_mtcars),
             format = "file")
)
```

To drive the point home, I use a function called `path_data()` which takes a
path as an input and simply returns it. This is totally superfluous, and you
could define the target like this instead:

```{r, eval = F}
tar_target(
  path_data_mtcars,
  "mtcars.csv",
  format = "file"
)
```

This would have exactly the same effect as using the `path_data()` function.

So now we got a target called `path_data_mtcars` that returns nothing but the
path to the data. But because we’ve used the `format = "file"` option,
`{targets}` now knows that this is a file that must be tracked. So any change on
this file will be correctly recognized and any target that depends on this input
file will be marked as being out-of-date. The other targets are exactly the
same.

Run the pipeline now using `targets::tar_make()`. Now, change the input file again:

```{r, eval = F}
write.csv(head(mtcars),
          "mtcars.csv",
          row.names = F)
```

Now, run the pipeline again using `targets::tar_make()`: this time you should
see that `{targets}` correctly identified the change and runs the pipeline again
accordingly!

## The dependency graph

As you’ve seen in the previous section (and as I told you in the introduction)
`{targets}` keeps track of changes in files, but also in the functions that you use.
Any change in any of these files will result in `{targets}` identifying which targets
are now out-of-date and which should be re-computed (alongside any other target that
depends on them). It is possible to visualise this using `targets::tar_visnetwork()`.
This opens an interactive network graph in your web browser that looks like this:

<figure>
    <img src="images/targets_visnetwork.png"
         alt="This image opens in your web-browser."></img>
    <figcaption>This image opens in your web-browser.</figcaption>
</figure>

In the image above, each target has been computed, so they are all up-to-date.
If we now change the input data as before, here is what you will see instead:

<figure>
    <img src="images/targets_visnetwork_outdated.png"
         alt="Because the input data got changed, we need to run the pipeline again."></img>
    <figcaption>Because the input data got changed, we need to run the pipeline again.</figcaption>
</figure>

Because all the targets depend on the input data, we need to re-run everything.
Let's run the pipeline again to update all the targets using `tar_make()`. 

Now let's add another target to our pipeline, one that does not depend on the
input data. Then, we will modify the input data again, and call
`tar_visnetwork()` as well. Change the pipeline like so:

```{r, eval = F}
list(
  tar_target(
    path_data_mtcars,
    "mtcars.csv",
    format = "file"
  ),
  tar_target(
    iris_data,
    data("iris")
  ),
  tar_target(
    summary_iris,
    summary(iris_data)
  ),
  tar_target(
    data_mtcars,
    read.csv(path_data_mtcars)
  ),
  tar_target(
    summary_mtcars,
    summary(data_mtcars)
  ),
  tar_target(
    plot_mtcars,
    save_plot(filename = "mtcars_plot.png",
              data_mtcars),
    format = "file")
)
```

Before running the pipeline, we can call `targets::tar_visnetwork()` again to
see the entire workflow:

<figure>
    <img src="images/targets_visnetwork_iris.png"
         alt="We clearly see that the pipeline has two completely independent parts."></img>
    <figcaption>We clearly see that the pipeline has two completely independent parts.</figcaption>
</figure>

We can see that there are now two independent parts, as well as two unused
functions, `path_data()` and `summ()` which we could remove.

Running the pipeline using `targets::tar_make()` builds everything
successfully. Let’s add the following target, just before the very
last one:

```{r, eval = F}
tar_target(
  list_summaries,
  list(
    "summary_iris" = summary_iris,
    "summary_mtcars" = summary_mtcars
  )
),
```

This target creates a list with the two summaries that we compute. Call
`targets::tar_visnetwork()` again:

<figure>
    <img src="images/targets_visnetwork_list_summaries.png"
         alt="The two separate workflows end up in one output."></img>
    <figcaption>The two separate workflows end up in one output.</figcaption>
</figure>

Finally, run the pipeline one last time to compute the final output.

## Running the pipeline in parallel

`{targets}` make it easy to run independent parts of our pipeline in parallel.
In the example from before, it was quite obvious to know which parts were
independent, but when the pipeline grows in complexity, it can be very difficult
to see which parts are independent.

Let’s now run the example from before in parallel. But first, we need to create
a function that takes some time to run. `summary()` is so quick that running
both of its calls in parallel is not worth it (and would actually even run
slower, I’ll explain why at the end). Let’s define a new function called
`slow_summary()`:

```{r, eval = F}
slow_summary <- function(...){
  Sys.sleep(30)
  summary(...)
}
```

and replace every call to `summary()` with `slow_summary()` in the
pipeline:

```{r, eval = F}
list(
  tar_target(
    path_data_mtcars,
    "mtcars.csv",
    format = "file"
  ),
  tar_target(
    iris_data,
    data("iris")
  ),
  tar_target(
    summary_iris,
    slow_summary(iris_data)
  ),
  tar_target(
    data_mtcars,
    read.csv(path_data_mtcars)
  ),
  tar_target(
    summary_mtcars,
    slow_summary(data_mtcars)
  ),
  tar_target(
    list_summaries,
    list(
      "summary_iris" = summary_iris,
      "summary_mtcars" = summary_mtcars
    )
  ),
  tar_target(
    plot_mtcars,
    save_plot(filename = "mtcars_plot.png",
              data_mtcars),
    format = "file")
)
```

here’s what the pipeline looks like before running:

<figure>
    <img src="images/targets_visnetwork_slow_summary.png"
         alt="slow_summary() is used instead of summary()."></img>
    <figcaption>slow_summary() is used instead of summary().</figcaption>
</figure>

(You will also notice that I’ve removed the unneeded functions, `path_data()`
and `summ()`).

Running this pipeline sequentially will take about a minute. To re-run
the pipeline completely from scratch, call `targets::tar_destroy()`. This
will make all the targets outdated. Then, run the pipeline from scratch
with `targets::tar_make()`:

```{r, eval = F}
targets::tar_make()
```

```

• start target path_data_mtcars
• built target path_data_mtcars [0.18 seconds]
• start target iris_data
• built target iris_data [0 seconds]
• start target data_mtcars
• built target data_mtcars [0 seconds]
• start target summary_iris
• built target summary_iris [30.26 seconds]
• start target plot_mtcars
• built target plot_mtcars [0.16 seconds]
• start target summary_mtcars
• built target summary_mtcars [30.29 seconds]
• start target list_summaries
• built target list_summaries [0 seconds]
• end pipeline [1.019 minutes]
```

But because computing `summary_iris` is completely independent of 
`summary_mtcars`, these two computations could be running at the same
time on two separate CPU cores. To do this, we need to first load
two additional packages, `{future}` and `{future.callr}` at the top of
the script. Then, we also need to call `plan(callr)` before defining
our pipeline. Here is what the complete `_targets.R` looks like:

```{r, eval = F}
library(targets)
library(future)
library(future.callr)
plan(callr)

# Sometimes you gotta take your time
slow_summary <- function(...) {
  Sys.sleep(30)
  summary(...)
}

# Save plot to disk
save_plot <- function(filename, ...){
  png(filename = filename)
  plot(...)
  dev.off()

  filename
}

# Set target-specific options such as packages.
tar_option_set(packages = "dplyr")

list(
  tar_target(
    path_data_mtcars,
    "mtcars.csv",
    format = "file"
  ),
  tar_target(
    iris_data,
    data("iris")
  ),
  tar_target(
    summary_iris,
    slow_summary(iris_data)
  ),
  tar_target(
    data_mtcars,
    read.csv(path_data_mtcars)
  ),
  tar_target(
    summary_mtcars,
    slow_summary(data_mtcars)
  ),
  tar_target(
    list_summaries,
    list(
      "summary_iris" = summary_iris,
      "summary_mtcars" = summary_mtcars
    )
  ),
  tar_target(
    plot_mtcars,
    save_plot(
      filename = "mtcars_plot.png",
      data_mtcars),
    format = "file")
)
```

You can now run this pipeline in parallel using `targets::tar_make_future()`
(and sequentially as well, just as usual with `targets::tar_make()`). Don’t
forget to run `targets::tar_destroy()` to run your pipeline from 0 again:

```
# Set workers = 2 to use 2 cpu cores
targets::tar_make_future(workers = 2)
```

```
• start target path_data_mtcars
• start target iris_data
• built target path_data_mtcars [0.2 seconds]
• start target data_mtcars
• built target iris_data [0.22 seconds]
• start target summary_iris
• built target data_mtcars [0.2 seconds]
• start target plot_mtcars
• built target plot_mtcars [0.35 seconds]
• start target summary_mtcars
• built target summary_iris [30.5 seconds]
• built target summary_mtcars [30.52 seconds]
• start target list_summaries
• built target list_summaries [0.21 seconds]
• end pipeline [38.72 seconds]
```

As you can see, this was faster but not quite twice as fast, but faster
nonetheless. The reason this isn’t exactly twice as fast is because there is
some overhead to run code in parallel. New R session have to be spawned by
`{targets}` and data needs to be transferred, and packages loaded, in these new
sessions. This is why it’s only worth parallelizing code that takes some time to
run. If you decrease the number of sleep seconds in `slow_summary(...)` (for
example to 10), running the code in parallel might even be slower, because of
that overhead. But if you have several long-running computations, it’s really
worth the very small price that you pay for the initial setup.

## {targets} and Quarto

## {targets} and {renv} 

*Why build automation: removes cognitive load, is a form of documentation in and of itself, as
Miles said*

*It is possible to communicate a great deal of domain knowledge in code, such that it is illuminating beyond the mere mechanical number crunching. To do this well the author needs to make use of certain styles and structures that produce code that has layers of domain specific abstraction a reader can traverse up and down as they build their understanding of the project. Functional programming style, coupled with a dependency graph as per {targets} are useful tools in this regard.*

## Rewriting our project as a pipeline

## Common mistakes

```

targets::tar_make()
Error:
! Error running targets::tar_make()
  Target errors: targets::tar_meta(fields = error, complete_only = TRUE)
  Tips: https://books.ropensci.org/targets/debugging.html
  Last error: argument 9 is empty
```
