# Build automation

We are finally ready to actually build a pipeline. For this, we are going to be
using a package called `{targets}` which is a so-called build automation tool.

If you go back to the reproducibility iceberg, you will see that we are quite
low now.

Without a build automation tool, a pipeline is nothing but a series of scripts
that get called one after the other, or perhaps the pipeline is only one very
long script that does the required operations successfully.

There are several problems with this approach.

## Introduction

Scripts can, and will, be executed out of order. You can mitigate the problems
this can make by using pure functions, but even then, you risk messing things up
when you do this. But why would you run your script out of order? Well, usually
because you changed a function, and only want to re-execute the parts of the
pipeline that are impacted by that change. But this supposes that you can know,
in your head, which part of the script was impacted and which was not. And this
can be quite difficult to figure out, especially when the pipeline is huge.

Another issue, but this one is perhaps more subjective, is that pipelines
written as scripts are usually quite difficult to read and understand. To
mitigate this, what you'd typically do is write a lot of comments. But here
again you face the problem of needing to maintain these comments, and once the
comments and the code are out of synch... the problems start.

Running the different parts of the pipeline in parallel is also very complicated
if your pipeline is defined as script. You would need to break the script into
independent parts (and make really sure that these parts are independent) and
execute them in parallel, perhaps using a separate R session for each script.
The good news is that if you followed the advice from this book is that you have
been using functional programming and so your pipeline is a series of function
calls, which is "easy" to break up and run in parallel.

But by now you should now that software engineers also faced similar problems
when they needed to build their software, and you should also know that they
likely came up with something to alleviate these issues. Enter build automation
tools.

When using a build automation tool, what you end up doing is writing down a
recipe, that will not be very different than a standard script in your favourite
programming language, that defines how the source code should be "cooked" into
the software (or in our case, a report, a cleaned dataset or any data product).

The build automation tool can then figure out the following things:

- any change in any of the code. Only the outputs that are affected by the changes you did will be re-computed (and their dependencies as well);
- any change in any of the tracked files. For example, if a file gets updated daily, you can track this file and the build automation tool will only execute the parts of the pipeline affected by this update;
- which parts of the pipeline can safely run in parallel (with the option to thus run the pipeline on multiple CPU cores).

Just like many of the other tools that we have encountered in this book, what
build automation tools do is allow you to not have to rely on your brain. You
*write down* the recipe once, and then you can focus again, on just the code of
your actual project. You shouldn't have to think about the pipeline itself, nor
think about to best run it. Let your computer figure that out for you, it's much
better at such tasks than you.

## {targets} quick-start

First thing's first: to know everything about the `{targets}` package, you
should read the excellent [`{targets}`
manual](https://books.ropensci.org/targets/)^[https://is.gd/VS6vSs]. 
Everything's in there. So what I'm going to do is really to just give you 
a very quick intro to what I think are really the main points that you should
know about.

Let's start by a "hello-world" type pipeline. Create a new folder called
something like `targets_intro/`, and start a fresh R session in it. For now, let’s
ignore `{renv}`. We will see how `{renv}` works together with `{targets}` to provide
a reproducible pipeline later. In that fresh session inside the `targets_intro/`
run the following line:

```{r, eval = FALSE}
targets::tar_script()
```

this will create a template `_targets.R` file in that directory. This is the file
in which we will define our pipeline. Open in it in your favourite editor. A
`_targets.R` pipeline is roughly divided into three parts:

- first is where packages are loaded and helper functions defined;
- second is where pipeline-specific options are defined;
- third is the pipeline itself, defined as a series of targets.

Let’s go through all these parts one by one.

### _targets.R’s anatomy

The first part of the pipeline is where package get loaded as well as helper
functions. In the template, the very first line is a `library(targets)` call
and then a function definition. There are two important things here that you need
to understand.

If your pipeline needs, say, the `{dplyr}` package to run, you could write
`library(dplyr)` in there. However, it is best to actually do as in the
template, and load the packages using `tar_option_set(packages = "dplyr")`. This
is because is you execute the pipeline in a cluster, you need to make sure that
all the packages are available to all the workers. If you load the packages at
the top of the `_targets.R` script, the packages will be available for the
session that called `library(...)`, but not any worker sessions spawned for
parallel execution. 
  
So, the idea is that at the very top of your script, you only load the
`{targets}` library and other packages that are required for running the
pipeline itself (as we shall see in coming sections). But packages that are
required by functions that are running inside the pipeline, these should ideally
be loaded as in the template. Another way of saying this: at the top of the
script, think "pipeline infrastructure" packages (`{targets}` and some others),
but inside `tar_option_set()` think "functions that run inside the pipeline"
packages.

Part two is where you set some global options for the pipeline. As discussed
previously, this is where you should load packages that are required by the
functions that are called inside the pipeline. I won’t list all the options
here, because I would simply be repeating what’s in the
[documentation](https://docs.ropensci.org/targets/reference/tar_option_set.html)^[https://is.gd/lm4QoO].
This second part is also where you can define some functions that you might need
for running the pipeline. For example, you might need to define a function to
load and clean some data: this is where you would do so. We have developed a
package, so we might not need this. But sometimes your analysis doesn’t require
you to write any custom functions, or maybe just a few, and perhaps you don’t
see the benefit of building a package just for one or two functions. So instead,
you have two other options: you either define them directly inside the
`_targets.R` script, like in the template, or you create a `functions/` folder
next to the `_targets.R` script, and put your functions there. It’s up to you,
but I prefer this second option.

Finally, comes the pipeline itself. Let’s take a closer look at it:

```{r, eval = F}
list(
  tar_target(data, data.frame(x = sample.int(100), y = sample.int(100))),
  tar_target(summary, summ(data)) # Call your custom functions as needed.
)
```

The pipeline is nothing but a list (told you lists where a very important
object) of "targets". A target is defined using the `targets::tar_target()`
function and has at least two inputs: the first is the name of the target
(without quotes) and the second is the function that generates the target. So a
target defined as `tar_target(y, f(x))` is equivalent to `y <- f(x)`. The next
target can use the output of the previous target as an input, so you could have
something like `tar_target(z, f(y))` (just like in the template). 

## A pipeline is a composition of pure functions

This pipeline can immediately be run using the `targets::tar_make()` command in
a console. Doing so shows you the following output:

```{r, eval = F}
targets::tar_make()
```

```
• start target data
• built target data [0.82 seconds]
• start target summary
• built target summary [0.02 seconds]
• end pipeline [1.71 seconds]
```

The pipeline is done running! So, now what? This pipeline simply built some
summary statistics, but where are they? Typing `summary` in the console to try
to inspect this output results in the following:

```{r, eval = F}
summary
```

```
function (object, ...) 
UseMethod("summary")
<bytecode: 0x000001f1a5436d78>
<environment: namespace:base>
```

What is going on?

First, you need to remember our chapter on functional programming. We want our
pipeline to be a sequence of pure functions. This means that our pipeline
running successfully should not depend on anything in the global environment
(apart from loading the packages in the first part of the script, and why we
should use `tar_option_set()` for the others) and it should not change anything
outside of its scope. This means that the pipeline should not change anything in
the global environment either. This is exactly how a `{targets}` pipeline
operates. A pipeline defined using `{targets}` will be pure and so the output of
the pipeline will not be save in the global environment. Now, strictly speaking,
the pipeline is not exactly pure. Check the folder that contains the
`_targets.R` script. There should now be a `_targets/` folder in there as well.
If you go inside that folder, and then open the `objects/` folder, you should
see two objects, `data` and `summary`. These are the outputs of our pipeline.

So each target that is defined inside the pipeline gets saved there in the
`.rds` format. This is an R-specific format that can be used to save *any* type
of object. It doesn’t matter what it is: a simple data frame, a fitted model, a
ggplot, whatever, any object can be saved into this format using the `saveRDS()`
function, and can be read back into another R session using `readRDS()`.
`{targets}` makes use of these two functions to save every target. Keep this in
mind if you use Git to version the code of your pipeline (which you are doing of
course), and add the `targets/` folder to the `.gitignore` (unless you really
want to also version it, but it shouldn’t be necessary).

Ok, so back to `summary`. What happened before when we typed `summary` in the
console? Well, now you know why the result didn’t appear: this is because the
target called `summary` gets saved in that folder, but not in the global
environment. So why did we see something anyways, which was not looking like a
data summary at all?

This is because R has a `summary()` function. So when you write the function’s name
in the console without `()` you see the function’s source code. Let’s try again:

```{r, eval = F}
summary
```


```{r, eval = F}
function (object, ...) 
UseMethod("summary")
<bytecode: 0x000001f1a5436d78>
<environment: namespace:base>
```

Ok, so here, strictly speaking, you don’t see the source code from `summary()`.
This is because `summary()` is a generic function, that simply picks the right
`summary()` depending to what type of object you pass to it. Try with
`summary.data.frame()` and there you shall see the source code. So when you type
`summary` after running the pipeline, because the pipeline is pure and nothing
gets saved to the global environment, you see `summary`’s source code and not
the target that got computed by the pipeline.

So that was a long explanation, but I think that it was worth the effort. It is
quite important to not forget that a targets pipeline is the composition of many
pure functions, but since we need to be able to access the outputs produced by
the pipeline, they also get saved inside a folder. To retrieve the outputs you
can use `targets::tar_read()` or `targets::tar_load()`. The difference is that
`tar_read()` simply reads the output and shows it in the console (just like when
you type `mtcars` in a console for example) but `tar_load()` reads and saves the
object into the global environment (just like doing something like `x <- mtcars`
in an R console). So to retrieve our `summary` object let’s use
`tar_load(summary)`:

```{r, eval = F}
tar_load(summary)
```

Now, typing `summary` shows the computed output (now the function
`base::summary()` is masked):

```{r, eval = F}
summary
```

```{r, eval = F}
    mean_x
1 500000.5
```

it is possible to load all the outputs using `targets::tar_load_everything()` so
that you don’t need to load each output one by one.

Before continuing with more `{targets}` features, I want to really stress the
fact the pipeline is the composition of pure functions. So functions that only
have a side-effect will be difficult to handle. For example, plotting in base R
consists in a series of calls to functions with side-effects. If you open an R
console and type `plot(mtcars)`, you will see a plot. But the function `plot()`
does not create any output. It just prints a picture on your screen, which is a
side-effect. To convince yourself that `plot()` does not create any output and only
has a side-effect, try to save the output of `plot()` in a variable:

```{r, eval = F}
a <- plot(mtcars)
```

doing this will show the plot, but then if you call `a`, the plot will not appear, and instead
you will get NULL:


```{r, eval = F}
a
```

```{r, eval = F}
NULL
```

This is also why saving plots in R is awkward, it’s because there’s no object to
actually save!

So because `plot()` is not a pure function, if you try to use it in a
`{targets}` pipeline, you will get `NULL` as well when loading the target. To
see this, change the list of targets like this:

```{r, eval = F}
list(
  tar_target(data, data.frame(x = sample.int(100), y = sample.int(100))),
  tar_target(summary, summ(data)),
  tar_target(data_plot, plot(data))
)
```

I’ve simply added a new target using `tar_target()`. Run the pipeline again
using `tar_make()` and then type `tar_load(data_plot)` to load the `data_plot`
target. But typing `data_plot` only shows `NULL` and not the plot!

There are several workarounds for this. The first is to use `ggplot()` instead.
This is because the output of `ggplot()` is an object of type `ggplot`. You can
do something like `a <- ggplot()` and then type `a` to see an empty canvas.
Doing `str(a)` also shows an output (the structure of the object a of class
`ggplot`).

The second workaround is to save the plot to disk. For this, you need to write a
new function, for example:

```{r, eval = F}
save_plot <- function(filename, ...){

  png(filename = filename)
  plot(...)
  dev.off()

}
```

If you put this in the `_targets.R` script, before defining the list of `tar_target` objects, 
you could use this instead of `plot()` in the last target:

```{r, eval = F}
summ <- function(dataset) {
  summarize(dataset, mean_x = mean(x))
}

save_plot <- function(filename, ...){
  png(filename = filename)
  plot(...)
  dev.off()

  filename
}

# Set target-specific options such as packages.
tar_option_set(packages = "dplyr")

# End this file with a list of target objects.
list(
  tar_target(data,
             data.frame(x = sample.int(100),
                        y = sample.int(100))),
  tar_target(summary,
             summ(data)),

  tar_target(data_plot,
             save_plot(filename = "my_plot.png",
                       data),
             format = "file")
)
```

After running this pipeline you should see a file called `my_plot.png` in the
folder of your pipeline. If you type `tar_load(data_plot)`, and then `data_plot`
you will see that this target returns the `filename` argument of `save_plot()`.
This is because a target needs to return something, and in the case of functions
that save a file to disk returning the path is recommended. This is because if I
then need to use this file in another target, I could do `tar_target(x,
f(data_plot))`. Because the `data_plot` target returns a path, I can write `f()`
in such a way that it knows how to handle this path. If instead I write
`tar_target(x, f("path/to/my_plot.png"))`, then `{targets}` would have no way of
knowing that the target `x` depends on the target `data_plot`. The dependency
between these two targets would break. Hence why the first option is preferable.

Finally, you will have noticed that the last target also has the option `format =
"file"`. This will be topic of the next section.

## Handling files

In this section, we will learn how `{targets}` handles files. First, run the following
lines in the folder that contains the `_targets.R` script that we’ve been using up until
now:

```{r, eval = F}
data(mtcars)

write.csv(mtcars, "mtcars.csv", row.names = F)
```

This will create the file `"mtcars.csv"` in that folder. We are going to use
this in our pipeline.

Write the pipeline like this:

```{r, eval = F}
list(
  tar_target(
    data_mtcars,
    read.csv("mtcars.csv")
  ),
  tar_target(
    summary_mtcars,
    summary(data_mtcars)
  ),
  tar_target(plot_mtcars,
             save_plot(filename = "mtcars_plot.png",
                       data_mtcars),
             format = "file")
)
```

You can now run the pipeline and will get a plot at the end. 
The problem however, is that the input file `"mtcars.csv"` is not being 
tracked for changes. Try to change the file, for example by running this line:

```{r, eval = F}
write.csv(head(mtcars), "mtcars.csv", row.names = F)
```

If you try to run the pipeline again, these changes are ignored:

```
✔ skip target data_mtcars
✔ skip target plot_mtcars
✔ skip target summary_mtcars
✔ skip pipeline [0.1 seconds]
```

As you can see, because `{targets}` is not tracking the changes in the `mtcars.csv` file,
from the its point of view nothing changed. And thus the pipeline gets skipped because 
according to `{targets}`, it is up-to-date.

Let’s change the csv back:

```{r, eval = F}
write.csv(mtcars, "mtcars.csv", row.names = F)
```

and change the first target such that the file gets tracked. Remember that
targets need to be pure functions and return something. So we are going to
change the first target to simply return the path to the file, and use the
`format = "file"` option in `tar_target()`:

```{r, eval = F}
path_data <- function(path){
  path
}

list(
  tar_target(
    path_data_mtcars,
    path_data("mtcars.csv"),
    format = "file"
  ),
  tar_target(data_mtcars,
             read.csv(path_data_mtcars)
             ),
  tar_target(
    summary_mtcars,
    summary(data_mtcars)
  ),
  tar_target(plot_mtcars,
             save_plot(filename = "mtcars_plot.png",
                       data_mtcars),
             format = "file")
)
```

To drive the point home, I use a function called `path_data()` which takes a
path as an input and simply returns it. This is totally superfluous, and you
could define the target like this instead:

```{r, eval = F}
tar_target(
  path_data_mtcars,
  "mtcars.csv",
  format = "file"
)
```

This would have exactly the same effect as using the `path_data()` function.

So now we got a target called `path_data_mtcars` that returns nothing but the
path to the data. But because we’ve used the `format = "file"` option,
`{targets}` now knows that this is a file that must be tracked. So any change on
this file will be correctly recognized and any target that depends on this input
file will be marked as being out-of-date. The other targets are exactly the
same.

Run the pipeline now using `targets::tar_make()`. Now, change the input file again:

```{r, eval = F}
write.csv(head(mtcars),
          "mtcars.csv",
          row.names = F)
```

Now, run the pipeline again using `targets::tar_make()`: this time you should
see that `{targets}` correctly identified the change and runs the pipeline again
accordingly!

## The dependency graph

As you’ve seen in the previous section (and as I told you in the introduction)
`{targets}` keeps track of changes in files, but also in the functions that you use.
Any change in any of these files will result in `{targets}` identifying which targets
are now out-of-date and which should be re-computed (alongside any other target that
depends on them). It is possible to visualise this using `targets::tar_visnetwork()`.
This opens an interactive network graph in your web browser that looks like this:

<figure>
    <img src="images/targets_visnetwork.png"
         alt="This image opens in your web-browser."></img>
    <figcaption>This image opens in your web-browser.</figcaption>
</figure>

In the image above, each target has been computed, so they are all up-to-date. If we now change
the input data as before, here is what you will see instead:

<figure>
    <img src="images/targets_visnetwork_outdated.png"
         alt="Because the input data got changed, we need to run the pipeline again."></img>
    <figcaption>Because the input data got changed, we need to run the pipeline again.</figcaption>
</figure>

Because all the targets depend on the input data, we need to re-run everything.
Let's run the pipeline again to update all the targets using `tar_make()`. Now
let's add another target to our pipeline, one that does not depend on the input
data. Then, we will modify the input data again, and call `tar_visnetwork()` as
well.



## Running the pipeline in parallel

## {targets} and {renv}

*Why build automation: removes cognitive load, is a form of documentation in and of itself, as
Miles said*

*It is possible to communicate a great deal of domain knowledge in code, such that it is illuminating beyond the mere mechanical number crunching. To do this well the author needs to make use of certain styles and structures that produce code that has layers of domain specific abstraction a reader can traverse up and down as they build their understanding of the project. Functional programming style, coupled with a dependency graph as per {targets} are useful tools in this regard.*

## Rewriting our project as a pipeline
