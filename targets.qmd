# Build automation

We are finally ready to actually build a pipeline. For this, we are going to be
using a package called `{targets}` which is a so-called build automation tool.

Without a build automation tool, a pipeline is nothing but a series of scripts
that get called one after the other, or perhaps the pipeline is only one
very long script that does the required operations successfully.

There are several problems with this approach.

## Introduction

Scripts can, and will, be executed out of order. You can mitigate the problems
this can make by using pure functions, but even then, you risk messing things up
when you do this. But why would you run your script out of order? Well, usually
because you changed a function, and only want to re-execute the parts of the
pipeline that are impacted by that change. But this supposes that you can know,
in your head, which part of the script was impacted and which was not. And this
can be quite difficult to figure out, especially when the pipeline is huge.

Another issue, but this one is perhaps more subjective, is that pipelines
written as scripts are usually quite difficult to read and understand. To
mitigate this, what you'd typically do is write a lot of comments. But here
again you face the problem of needing to maintain these comments, and once the
comments and the code are out of synch... the problems start.

Running the different parts of the pipeline in parallel is also very complicated
if your pipeline is defined as script. You would need to break the script into
independent parts (and make really sure that these parts are independent) and
execute them in parallel, perhaps using a separate R session for each script.
The good news is that if you followed the advice from this book is that you have
been using functional programming and so your pipeline is a series of function
calls, which is "easy" to break up and run in parallel.

But by now you should now that software engineers also faced similar problems
when they needed to build their software, and you should also know that they
likely came up with something to alleviate these issues. Enter build automation
tools.

When using a build automation tool, what you end up doing is writing down a
recipe, that will not be very different than a standard script in your favourite
programming language, that defines how the source code should be "cooked" into
the software (or in our case, a report, a cleaned dataset or any data product).

The build automation tool can then figure out the following things:

- any change in any of the code. Only the outputs that are affected by the changes you did will be re-computed (and their dependencies as well);
- any change in any of the tracked files. For example, if a file gets updated daily, you can track this file and the build automation tool will only execute the parts of the pipeline affected by this update;
- which parts of the pipeline can safely run in parallel (with the option to thus run the pipeline on multiple CPU cores).

Just like many of the other tools that we have encountered in this book, what
build automation tools do is allow you to not have to rely on your brain. You
*write down* the recipe once, and then you can focus again, on just the code of
your actual project. You shouldn't have to think about the pipeline itself, nor
think about to best run it. Let your computer figure that out for you, it's much
better at such tasks than you.

## {targets} quick-start

First thing's first: to know everything about the `{targets}` package, you
should read the excellent [`{targets}`
manual](https://books.ropensci.org/targets/)^[https://is.gd/VS6vSs]. 
Everything's in there. So what I'm going to do is really to just give you 
a very quick intro to what I think are really the main points that you should
know about.

Let's start by a "hello-world" type pipeline. Create a new folder called
something like `targets_intro/`, and start a fresh R session in it. For now, let’s
ignore `{renv}`. We will see how `{renv}` works together with `{targets}` to provide
a reproducible pipeline later. In that fresh session inside the `targets_intro/`
run the following line:

```{r, eval = FALSE}
targets::tar_script()
```

this will create a template `_targets.R` file in that directory. This is the file
in which we will define our pipeline. Open in it in your favourite editor. A
`_targets.R` pipeline is roughly divided into three parts:

- first is where packages are loaded and helper functions defined;
- second is where pipeline-specific options are defined;
- third is the pipeline itself, defined as a series of targets.

Let’s go through all these parts one by one.

### _targets.R’s anatomy

The first part of the pipeline is where package get loaded as well as helper
functions. In the template, the very first line is a `library(targets)` call
and then a function definition. There are two important things here that you need
to understand.

If your pipeline needs, say, the `{dplyr}` package to run, you could write
`library(dplyr)` in there. However, it is best to actually do as in the
template, and load the packages using `tar_option_set(packages = "dplyr")`. This
is because is you execute the pipeline in a cluster, you need to make sure that
all the packages are available to all the workers. If you load the packages at
the top of the `_targets.R` script, the packages will be available for the
session that called `library(...)`, but not any worker sessions spawned for
parallel execution. 
  
So, the idea is that at the very top of your script, you only load the
`{targets}` library and other packages that are required for running the
pipeline itself (as we shall see in coming sections). But packages that are
required by functions that are running inside the pipeline, these should ideally
be loaded as in the template. Another way of saying this: at the top of the
script, think "pipeline infrastructure" packages (`{targets}` and some others),
but inside `tar_option_set()` think "functions that run inside the pipeline"
packages.

Part two is where you set some global options for the pipeline. As discussed
previously, this is where you should load packages that are required by the
functions that are called inside the pipeline. I won’t list all the options
here, because I would simply be repeating what’s in the
[documentation](https://docs.ropensci.org/targets/reference/tar_option_set.html)^[https://is.gd/lm4QoO].
This second part is also where you can define some functions that you might need
for running the pipeline. For example, you might need to define a function to
load and clean some data: this is where you would do so. We have developed a
package, so we might not need this. But sometimes your analysis doesn’t require
you to write any custom functions, or maybe just a few, and perhaps you don’t
see the benefit of building a package just for one or two functions. So instead,
you have two other options: you either define them directly inside the
`_targets.R` script, like in the template, or you create a `functions/` folder
next to the `_targets.R` script, and put your functions there. It’s up to you,
but I prefer this second option.

Finally, comes the pipeline itself. Let’s take a closer look at it:

```{r, eval = F}
list(
  tar_target(data, data.frame(x = sample.int(100), y = sample.int(100))),
  tar_target(summary, summ(data)) # Call your custom functions as needed.
)
```

The pipeline is nothing but a list (told you lists where a very important
object) of "targets". A target is defined using the `targets::tar_target()`
function and has at least two inputs: the first is the name of the target
(without quotes) and the second is the function that generates the target. So a
target defined as `tar_target(y, f(x))` is equivalent to `y <- f(x)`. The next
target can use the output of the previous target as an input, so you could have
something like `tar_target(z, f(y))` (just like in the template). 

This pipeline can immediately be run using the `targets::tar_make()` command in
a console. Doing so shows you the following output:

```{r, eval = F}
targets::tar_make()
```

```
• start target data
• built target data [0.82 seconds]
• start target summary
• built target summary [0.02 seconds]
• end pipeline [1.71 seconds]
```

The pipeline is done running! So, now what? This pipeline simply built some summary
statistics, but where are they? Typing `summary` in the console to try to inspect this
output results in the following:

```{r, eval = F}
summary
```

```
function (object, ...) 
UseMethod("summary")
<bytecode: 0x000001f1a5436d78>
<environment: namespace:base>
```

What is going on?

Ok, so first, you need to remember our chapter on functional programming. We
want our pipeline to be a sequence of pure functions. This means that our
pipeline running successfully should not depend on anything in the global
environment (apart from loading the packages in the first part of the script,
and why we should use `tar_option_set()` for the others) and it should not
change anything outside of its scope. This means that the pipeline should not
change anything in the global environment either. This is exactly how a
`{targets}` pipeline operates. A pipeline defined using `{targets}` will be pure
and so the output of the pipeline will not be save in the global environment.
Now, strictly speaking, the pipeline is not exactly pure. Check the folder that
contains the `_targets.R` script. There should now be a `_targets/` folder in
there as well. If you go inside that folder, and then open the `objects/`
folder, you should see two objects, `data` and `summary`. These are the outputs
of our pipeline.

So each target that is defined inside the pipeline gets saved there in the
`.rds` format. This is an R-specific format that can be used to save *any* type
of object. It doesn’t matter what it is: a simple data frame, a fitted model, a
ggplot, whatever, any object can be saved into this format using the `saveRDS()`
function, and can be read back into another R session using `readRDS()`.
`{targets}` makes use of these two functions to save every target. Keep this in
mind if you use Git to version the code of your pipeline (which you are doing of
course), and add the `targets/` folder to the `.gitignore` (unless you really
want to also version it, but it shouldn’t be necessary).

Ok, so back to `summary`. What happened before when we typed `summary` in the
console? Well, now you know why the result didn’t appear: this is because the
target called `summary` gets saved in that folder, but not in the global
environment. So why did we see something anyways, which was not looking like a
data summary at all?

This is because R has a `summary()` function. So when you write the function’s name
in the console without `()` you see the function’s source code. Let’s try again:

```{r, eval = F}
summary
```


```{r, eval = F}
function (object, ...) 
UseMethod("summary")
<bytecode: 0x000001f1a5436d78>
<environment: namespace:base>
```

Ok, so here, strictly speaking, you don’t see the source code from `summary()`.
This is because `summary()` is a generic function, that simply picks the right
`summary()` depending to what type of object you pass to it. Try with 
`summary.data.frame()` and there you shall see the source code. So when you type
`summary` after running the pipeline, because the pipeline is pure and nothing gets
saved to the global environment, you see `summary`’s source code and not the 
target that got computed by the pipeline.

So that was a long explanation. Let’s go back to our pipeline.

## Handling files

## Running the pipeline in parallel

## {targets} and {renv}

*Why build automation: removes cognitive load, is a form of documentation in and of itself, as
Miles said*

*It is possible to communicate a great deal of domain knowledge in code, such that it is illuminating beyond the mere mechanical number crunching. To do this well the author needs to make use of certain styles and structures that produce code that has layers of domain specific abstraction a reader can traverse up and down as they build their understanding of the project. Functional programming style, coupled with a dependency graph as per {targets} are useful tools in this regard.*

## Rewriting our project as a pipeline
