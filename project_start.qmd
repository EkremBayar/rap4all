# Project start

In this chapter, we are going to work together on a very simple project. This project will stay
with us until the end of the book. For now, we are going to keep it simple; our goal here is to get
an analysis done. We are going to download some data, analyze it, and write a little report. Then,
we are going to think about how to go from this analysis to something more robust: what's
important to understand, is that everything we will be doing to get our little report can be seen
as a *proto-pipeline*. We are going to download data, write functions, write comments, write some
tests along the way to make sure that everything is working well, and put everything into an R
Markdown source file that outputs the finalized document in one command line.

The idea is to take this, and make it more a truly reproducible pipeline. So one of the first steps
once we're done with the analysis is to understand why we are actually not done yet.

But for now, our main concern is to get our work done.

## Housing in Luxembourg

We are going to download data about house prices in Luxembourg. Luxembourg is a little Western
European country that looks like a shoe and is about the size of .98 Rhode Islands from the author
hails from. Did you know that Luxembourg was a constitutional monarchy, and not a kingdom like
Belgium, but a Grand-Duchy, and actually the last Grand-Duchy in the Word in the World? Also, what
you should know to understand what we will be doing is that the country of Luxembourg is divided
into Cantons, and each Cantons into Communes. Basically, if Luxembourg was the USA, Cantons would
be States and Communes would be Counties (or Parishes or Boroughs). What’s confusing is that
“Luxembourg” is also the name of a Canton, and of a Commune, which also has the status of a city
and is the capital of the country. So Luxembourg the country, is divided into Cantons, one of
which is called Luxembourg as well, cantons are divided into communes, and inside the canton of 
Luxembourg there's the commune of Luxembourg which is also the city of Luxembourg, sometimes
called Luxembourg-City, which is the capital of the country.

![Luxembourg is about as big as the US State of Rhode Island](images/lux_rhode_island.png)

What you should also know is that the population is about 645.000 as of writing (January 2023),
half of which are foreigners. Around 400.000 persons work in Luxembourg, of which half do not live
in Luxembourg; so every morning from Monday to Friday, 200.000 people enter the country to work,
and leave on the evening to go back to either Belgium, France or Germany, the neighboring
countries. As you can imagine, this puts enormous pressure on the transportation system and on the
roads, but also on the housing market; everyone wants to live in Luxembourg to avoid the horrible
daily commute, and everyone wants to live either in the capital city, or in the second largest
urban area in the south, in a city called Esch-sur-Alzette.

The plot below shows the value of the House Price Index through time for Luxembourg and the 
European Union:

```{r, echo = F}
#https://ec.europa.eu/eurostat/databrowser/bookmark/21530d9a-c423-4ab7-998a-7170326136cd?lang=en
housing_lu_eu <- read.csv("datasets/prc_hpi_a__custom_4705395_page_linear.csv.gz")

withr::with_package("ggplot2",
  {
    ggplot(data = housing_lu_eu) +
      geom_line(aes(y = OBS_VALUE, x = TIME_PERIOD, group = geo, colour = geo),
                size = 1.5) +
      labs(title = "House price and sales index (2010 = 100)",
           caption = "Source: Eurostat") +
      theme_minimal() +
      theme(legend.position = "bottom")
  }
  )

```

If you want to download the data, click [here](https://ec.europa.eu/eurostat/databrowser/view/PRC_HPI_A__custom_4705395/bookmark/table?lang=en&bookmarkId=21530d9a-c423-4ab7-998a-7170326136cd).

Let us paste the definition of the HPI in here (taken from the HPI's
[metadata](https://ec.europa.eu/eurostat/cache/metadata/en/prc_hpi_inx_esms.htm) page):

*The House Price Index (HPI) measures inflation in the residential property market. The HPI
captures price changes of all types of dwellings purchased by households (flats, detached houses,
terraced houses, etc.). Only transacted dwellings are considered, self-build dwellings are
excluded. The land component of the dwelling is included.*

So from the plot, we can see that the price of dwellings more than doubled between 2010 and 2021;
the value of the index is 214.81 in 2021 for Luxembourg, and 138.92 for the European Union
as a whole. 

There is a lot of heterogeneity though; the capital and the communes immediately next to the
capital are much more expensive that communes from the less urbanized north, for example. The south
of the country is also more expensive than the north, but not as much as the capital and
surrounding communes. Not only is price driven by hand demand, but also by scarcity; in 2021, .5%
of residents owned 50% of the buildable land for housing purposes (Source: *Observatoire de
l'Habitat, Note 29*, [archived download link](https://archive.org/download/note-29/note-29.pdf)).

Our project will be quite simple; we are going to download some data, supplied as an Excel file,
compiled by the Housing Observatory (*Observatoire de l'Habitat*), a service from the Ministry of
Housing, which monitors the evolution of prices in the housing market, among other useful services
like the identification of vacant lots for example. The advantage of their data when compared to 
Eurostat's data is that the data is disaggregated by commune. The disadvantage is that they only
supply nominal prices, and no index. Nominal prices are the prices that you read on price tags in shops.
The problem with nominal prices is that it is difficult to compare them through time. Ask yourself 
the following question: would you prefer to have had 500€ (or USDs) in 2003 or in 2023? You
probably would have preferred them in 2003, as you could purchase a lot more with 500€ then
than now. In fact, according to a random inflation calculator I googled, to match the 
purchasing power of $500 in 2003, you'd need to have $793 in 2023 (and I'd say that we find very
similar values for €). But it doesn't really matter if that calculation is 100% correct:
what matters is that the value of money changes, and comparisons through time are difficult, hence
why an index is quite useful. So we are going to convert these nominal prices to real prices. Real
prices take inflation into account and so allow us to compare prices through time. So we will
need to also get some data to achieve this.

So to summarise; our goal is to:

- Get data trapped inside an Excel file into a neat data frame;
- Convert nominal to real prices using a simple method;
- Make some tables and plots and call it a day (or will we?).

And we are going to make all of this inside a R Markdown file.

## Saving trapped data from Excel

Getting data from Excel into a tidy data frame can be very tricky. This is because very often,
Excel is used as some kind of dashboarding, or presentation tool. So data is made human-readable,
in contrast to machine readable. Let us quickly discuss this topic as it is essential to grasp
the difference between the two (and in our experience, a lot of collective pain inflicted to statisticians
and researchers could have been avoided if this concept was more well-known). The picture below
shows an Excel made for human consumption:

![An Excel file meant for human eyes](images/obs_hab_xlsx_overview.png)

So why is this file not machine-readable? Here are some issues:

- The table does not start in the top-left corner of the spreadsheet, which is where most importing tools expect it to be;
- The spreadsheet start with a head that contains an image and some text;
- Numbers are text and use "," as the thousands separator;
- You don't see it in the screenshot, but each year is in a separate sheet.

That being said, this one is still very nice, and going from this Excel to a tidy dataframe will
not be too difficult. In fact, we suspect that whoever made this Excel file is well aware of the
contradicting requirements of human and machine readable formatting of data, and strove to find a
compromise. Because more often than not, getting human readable data into a machine readable
formatting is a nightmare.

This is actually the file that we are going to use for our project, so if you want to follow along,
you can download it [here](datasets/vente-maison-2010-2021.xlsx) (downloaded on january 2023 from
the 
[luxembourguish open data portal](https://data.public.lu/en/datasets/prix-annonces-des-logements-par-commune/)).

Each sheet contains a dataset with the following columns:

- Commune: the commune
- Nombre d'offres: the total number of selling offers
- Prix moyen annoncé en Euros courants: Average selling price in nominal Euros
- Prix moyen annoncé au m2 en Euros courants: Average selling price in square meters in nominal Euros

For ease of presentation, we are going to show you each function here separately, but we'll be putting
everything together in a single Rmd file at the end. So first, let's read in the data. For this,
we will be using this function here:

```{r}
get_raw_data <- function(url = "https://github.com/b-rodrigues/rap4all/raw/master/datasets/vente-maison-2010-2021.xlsx"){
  #raw_data <- tempfile(fileext = ".xlsx")

  #download.file(url, raw_data)

  raw_data <- "c:/Users/LLP685/Downloads/vente-maison-2010-2021.xlsx"

  sheets <- readxl::excel_sheets(raw_data)

  read_clean <- function(..., sheet){
    readxl::read_excel(..., sheet = sheet) %>%
      mutate(year = sheet)
  }

  raw_data <- purrr::map(
                       sheets,
                       ~read_clean(raw_data,
                                   skip = 10,
                                   sheet = .)
                  ) %>%
  dplyr::bind_rows() %>%
  janitor::clean_names()

  raw_data %>%
    dplyr::rename(
             locality = commune,
             n_offers = nombre_doffres,
             average_price_nominal_euros = prix_moyen_annonce_en_courant,
             average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant,
             average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant
           ) %>%
    dplyr::mutate(locality = stringr::str_trim(locality)) %>%
    dplyr::select(year, locality, n_offers, dplyr::starts_with("average"))

}

```

Let's discuss each part of the function. This function has a single argument, the url pointing
towards the data. The data is hosted on this book's Github repository, but you could just as well
replace this url by the one from the Open Data Portal for example. The file gets downloaded to a
temporary file (created using `tempfile(fileext = ".xlsx")`, and the name of the sheets from the
file get extracted using `readxl::excel_sheets()`. This is our first dependency: here, we're going
to use the `package::function()` notation to show you clearly from where function come from, but in
practice, inside your Rmd file, you would be calling `library(readxl)` at the start of the Rmd
file.

Then we define a new function, called `read_clean()`. This function is a wrapper around the
`readxl::read_excel()` function. Remember, our source Excel Workbook contains one sheet per year,
starting with 2010. Also, remember this huge header on each sheet? We can skip all these useless
lines using `skip = 10`, which skips the first 10 lines of each sheet. Thankfully, this is
consistent across the sheets. The sheets are named "2010", "2011", etc. So we take advantage of
this to read in the required sheet, and then also add a new column with the year which is the name
of the sheet. So if we would use our function on one sheet, say, "2011", we would have the data
from the that sheet (because we skip the first 10 lines with the header) and a new column called
`year` with "2011" everywhere. We then map this function to the list of sheets, obtained using
`excel_sheets()` which results in a list of data frames. These data frames get merged into a single
data set using `dplyr::bind_rows()` which takes a list of datasets with same columns as an argument
and returns a single dataset. We also use the `janitor::clean_names()` function to automatically
clean column names. For example, `clean_names()` turns a column name titled "Nombre d'offres"
(French for "Total offers") into "nombre_doffres", making it easier to reference in code. Columns
names then get translated from French to English, white spaces get removed from the `commune`
column, and only the required columns get selected.

Running this function results in an almost tidy dataset:

```{r}
# We call get_raw_data() without arguments, since the correct url is provided as a default argument
raw_data <- get_raw_data()
```

Let's take a look at the data:

```{r}
str(raw_data)
```

There's a problem already: columns that should be of type numeric are of type character instead
(`average_price_nominal_euros` and `average_price_m2_nominal_euros`). There's also another issue, which 
you would eventually catch as you would be exploring the data: naming of the communes is not consistent.
Let's take a look:

```{r}
raw_data %>%
  filter(grepl("Luxembourg", locality)) %>%
  count(locality)

```

We can see that the city of Luxembourg is spelled in two different ways. It's the same with another
commune, Pétange:

```{r}
raw_data %>%
  filter(grepl("P.tange", locality)) %>%
  count(locality)

```

So sometimes it is spelled correctly, with an "é", sometimes not. Let's write some code to correct
this:

```{r}
raw_data <- raw_data %>%
    mutate(locality = ifelse(grepl("Luxembourg-Ville", locality),
                             "Luxembourg",
                             locality),
           locality = ifelse(grepl("P.tange", locality),
                             "Pétange",
                             locality)
           ) %>%
    mutate(across(starts_with("average"), as.numeric))

```

Now this is interesting -- converting the `average` columns to numeric resulted in some `NA` values.
Let's see what happened:

```{r}
raw_data %>%
  filter(is.na(average_price_nominal_euros))
```

It turns out that there are no prices for certain communes, but that we also have some rows
with garbage in there. Let's go back to the raw data to see what this is about:

![Always look at your data](images/obs_hab_xlsx_missing.png)

So it turns out that indeed, there are some rows that we need to remove. We can start by removing
rows where `locality` is missing. Then we have a row where `locality` is equal to "Total d'offres".
This is simply the total of every offer from every commune. We could keep that in a separate data
frame, or even remove it. Finally there's a row, the last one, that states the source of the data,
which we can remove. 

In the screenshot above, we see another row that we don't see in our filtered data frame: one where
`n_offers` is missing. This row gives the national average for columns
`average_prince_nominal_euros` and `average_price_m2_nominal_euros`. What we are going to do is 
create two datasets: one with data on communes, and the other on national prices. Let's first
remove the rows stating the sources:

```{r}
raw_data <- raw_data %>%
  filter(!grepl("Source", locality))
```

Let's now only keep the communes in our data:

```{r}
commune_level_data <- raw_data %>%
    filter(!grepl("nationale|offres", locality),
           !is.na(locality))
```

And let's create a dataset with the national data as well:

```{r}
country_level <- raw_data %>%
  filter(grepl("nationale", locality)) %>%
  select(-n_offers)

offers_country <- raw_data %>%
  filter(grepl("Total d.offres", locality)) %>%
  select(year, n_offers)

country_level_data <- full_join(country_level, offers_country) %>%
  select(year, locality, n_offers, everything()) %>%
  mutate(locality = "Grand-Duchy of Luxembourg")

```

Now the data looks clean, and we can start the actual analysis... or can we? Before proceeding,
it would be nice to make sure that we got every commune in there. For this, we need a list
of communes from Luxembourg. 
[Thankfully, Wikipedia has such a list.](https://en.wikipedia.org/wiki/List_of_communes_of_Luxembourg)

Let's scrape and save this list:

```{r}
current_communes <- "https://en.wikipedia.org/wiki/List_of_communes_of_Luxembourg" %>%
  rvest::read_html() %>%
  rvest::html_table() %>%
  purrr::pluck(1) %>%
  clean_names()

```

We scrape the table from the Wikipedia page using `{rvest}`. `rvest::html_table()` returns a list
of tables from the Wikipedia table, and then we use `purrr::pluck()` to keep the first table from
the website, which is what we need.

Let’s see if we have all the communes in our data:

```{r}
setdiff(unique(commune_level_data$locality), current_communes$commune)
```

We see many communes that are in our `commune_level_data`, but not in `current_communes`. There’s
one obvious reason: differences in spelling, for example, "Kaerjeng" in our data, but "Käerjeng" in
the table from Wikipedia. But there’s also a less obvious reason; since 2010, several communes
have merged into new ones. So there are communes that are in our data, say, in 2010 and 2011, but
disappear from 2012 onwards. So we need to do several things: first, get a list of all existing
communes from 2010 onwards, and then, harmonise spelling. Here again, we can use a list of 
Wikipedia:

```{r}
former_communes <- "https://en.wikipedia.org/wiki/Communes_of_Luxembourg#Former_communes" %>%  
  read_html() %>%
  html_table() %>%
  pluck(3) %>%
  clean_names() %>%
  filter(year_dissolved > 2009)

former_communes

```

As you can see, since 2010 many communes have merged to form new ones. We can now combine the list
of current and former communes, as well as harmonize their names:

```{r}
communes <- unique(c(former_communes$name, current_communes$commune))
# we need to rename some communes

# Different spelling of these communes between wikipedia and the data

communes[which(communes == "Clemency")] <- "Clémency"
communes[which(communes == "Redange")] <- "Redange-sur-Attert"
communes[which(communes == "Erpeldange-sur-Sûre")] <- "Erpeldange"
communes[which(communes == "Luxembourg-City")] <- "Luxembourg"
communes[which(communes == "Käerjeng")] <- "Kaerjeng"
communes[which(communes == "Petange")] <- "Pétange"
```

Let’s run our test again:

```{r}
setdiff(unique(commune_level_data$locality), communes)
```

Great! When we compare the communes that are in our data with every commune that has existed since
2010, we don’t have any commune that is unaccounted for. So are we done with cleaning the data? 
Yes, we can now actually start with analysing the data. But before that let’s now put this into
an Rmd file. This Rmd file will be dedicated to preparing the data only, so we will need to write
another one for the analysis proper. What we’re also going to do, is take this as an opportunity
to refactor the code: you might have noticed that we have not used functions a lot in the code
snippets before, so let’s use functions instead. 

You can click [here]("rmds/save_data.Rmd") to take a look at the finalized Rmd file. The advantage
of taking the time to clean up our code and put together an Rmd file, is that we now have some high
quality documentation about the whole cleaning process. This will also serve as a basis for later,
when we will actually turn our analysis into a package.

## Analysing the data

We are now going to analyse the data. The first thing we are going to do is compute a Laspeyeres
price index. This price index allows us to make comparisons through time; for example, the index at
year 2012 measures how much more expensive (or chaper) housing became relative to the base year
(2010). However, since we only have one good, this index becomes quite simple to compute: it is
nothing but the prices at year *t* divided by the prices in 2010 (if we had a basket of goods,
we would need to use the Laspeyeres index formula to compute the index at all periods).

For this section, we will perform a rather simple analysis. We will immediately show you the Rmd
file: take a look at it [here]("rmds/analyse_data.Rmd"). 


## Your project is done (?)

*So here the project is done, but actually it's just an Qmd file that gets compiled, so we would need
to explain why this is not enough, and motivate the readers to go the full way: developing packages,
using targets, and so on*
