# Advanced topics in reproducibility

If the book ended at the end of the previous chapter, it would have been titled
"Building analytical pipelines with R", because we have not ensured that the
pipeline we built is reproducible. We did our best though:

- we used functional and literate programming;
- we documented, tested and versioned the code;
- we used `{renv}` to record the dependencies of the project;
- the project is now a pipeline and re-running it is as easy as it can possibly get.

But there are still many variables that we need to consider. If we go back to
the reproducibility iceberg, you will notice that we can still go deeper. As the
code stands now, we did our best using programming paradigms and libraries, but
now we need to consider another tool, called Docker.

As already mentioned in the introduction and in Chapter 10, `{renv}` *only*
restores package versions. The R version used for the analysis only gets
recorded. So to make sure that the pipeline reproduces the same results, you'd
need to install the same R version that was used to build the pipeline
originally. But installing the right R version can be difficult sometimes; it
depends on the operating system you want to install it on, and how old a version
we’re talking about. Installing R 4.0 on Windows should be quite easy, but I
wouldn’t be very keen on trying to install R 2.15.0 (released on March 2012) on
a current Linux distribution.

Next comes the operating system on which the pipeline was developed. In
practice, this rarely matters, but there have been cases where the same code
produces different results on different operating systems, sometimes even
on different versions of the same operating system!

And finally, I believe that we are in a transition period when it comes to
hardware architecture. Apple will very likely completely switch over to an ARM
architecture with their Apple silicon CPUs (as of writing, the Mac Pro is the
only computer manufactured by Apple that doesn't use an Apple silicon CPU and
only because it was released in 2019) and it wouldn't surprise me if other
manufacturers follow suit and develop their own ARM cpus. 

So, as I explained in the previous chapter, we want our pipeline to be the
composition of pure functions. Nothing in the global environment (apart from
`{target}`-specific options) should influence the runs of the pipeline. But,
what about the environment R is running in? The R engine is itself running in
some kind of environment. This is what I've explained above: operating system
(and all the math libraries these ship that R relies on to run code) and
hardware and variables that need to be recond and/or frozen as much as possible.

Think about it this way: when you running a pure function `f()` of one argument
you think you do this:

```
f(1)
```

but actually what you're doing is:

```
f(1, "windows 10 - build 22H2 - patch 10.0.19045.2075", "intel x86_64 cpu i9-13900F", "R version 4.2.2")
```

and so on. `f()` is only pure as far as the R version currently running `f()` is
concerned. But everything else should also be taken into account! Remember, in
technical terms this means that our function is not referentially transparent.

And this is what Docker allows us to do: turn our the pipeline referentially
transparent, by freezing R's and the operating system's versions (and the CPU
architecture us well).

## What is Docker?

Let me first explain in very simple terms what Docker is.

So in very simply (and technically wrong) terms, Docker makes it easy to run a
Linux virtual machine (VM) on your computer. A VM is a computer within a
computer: the idea is that a you turn on your computer, start Windows (the
operating system you use everyday), but then start Ubuntu (a very popular Linux
distribution) as if it were any other app installed on your computer and use it
(almost) as you would normally. This is what a classic VM solution like
*Virtualbox* offers you. You can start and use Ubuntu interactively from within
Windows. This can be quite useful for testing purposes for example.

The way Docker differs from Virtualbox (or VMware) though is that it strips down
the VM to its bare essentials. There’s no graphical user interface for example,
and you will not (typically) use a Docker VM interactively. What you will do
instead is write down in a text file the specifications of the VM you want.
Let’s call this text file a *Dockerfile*. For example, you want the VM to be
based off Ubuntu. So that would be the first line in the Dockerfile. You then
want it to have R installed. So that would be the second line. Then you need to
install R packages, so you add those lines as well. Maybe you need to add some
system dependencies? Add them. Finally, you add the code of the pipeline that
you want to make reproducible as well.

Once you’re done, you have this text file, the Dockerfile, with a complete
recipe to generate a Docker VM. That VM is called an *image* (as as I said
previously it’s technically not a true VM, but let’s not discuss this). So you
have a text file, and this file helps you define and generate an image. Here,
you should already see a first advantage of using Docker over a more traditional
VM solution like Virtualbox: you can very easily write these Dockerfiles and
version them. You can easily start off from another Dockerfile from another
project and adapt it to your current project. And most importantly, because
everything is written down, it’s reproducible (but more on that at the end of
this chapter...).

Ok so you have this image. This image will be based on some Linux distribution,
very often Ubuntu. It comes with a specific version of Ubuntu, and you can add
to it a specific version of R. You can also download a specific version of all
the packages required for your project. You end with an environment that is
tailor-made for your project. You can then run the project with this Docker
image, and *always get exactly the same results, ever*. This is because,
regardless of how, where or when you will run this *dockerized* project, the
same version of R, with the same version of R packages, on the same Linux
distribution will be used to reproduce the results of your pipeline. By the way,
when you run a Docker image (as in, your executing your pipeline inside that
container) this now is refered as a Docker container.

So: a Dockerfile defines a Docker image, from which you can then run containers.
I hope that the pictures below will help. The first picture shows what happens
when you run the same pipeline on two different R versions and two different
operating systems:

<figure>
    <img src="images/without_docker.png"
         alt="Running a pipeline without Docker results (potentially) in different outputs."></img>
    <figcaption>Running a pipeline without Docker results (potentially) in different outputs.</figcaption>
</figure>

Take a close look at the output, you will notice it’s different!

Now, you run the same pipeline, which is now *dockerized*:

<figure>
    <img src="images/with_docker.png"
         alt="Running a pipeline with Docker results in the same outputs."></img>
    <figcaption>Running a pipeline with Docker results in the same outputs.</figcaption>
</figure>

Another way of looking at a Docker image: it’s an immutable sandbox, where the
rules of the game are always the same. It doesn’t matter where or when you run
this sandbox, the pipeline will always be executed in this same, well-defined
space. Because the pipeline runs on the same versions of R (and packages)
and on the same operating system defined within the Docker image, our pipeline
is now truly reproducible.

But why Linux though; why isn’t it possible to create Docker images based on
Windows or macOS? Remember in the introduction, where I explained what 
reproducibility is? I wrote:

> Open source is a hard requirement for reproducibility.

Open source is not just a requirement for the programming language used for
building the pipeline, but extends to the operating system that the pipeline
runs on as well. So the reason Docker uses Linux, is because you can use Linux
distributions like Ubuntu for free and without restrictions. There aren’t any
licenses that you need to buy or activate, and Linux distributions can be
customized for any use-case imaginable. Thus Linux distributions are the only
option available to Docker for this task.

## A primer on Linux

Up until this point, you could have followed along using any operating system.
Most of the code shown in this book is R code, so it doesn’t matter on what
operating system you’re running it. But there was some code that I ran in the
Linux console (for example, I’ve used `ls` to list files). These commands should
also work on macOS, and some even on Windows (`ls` also works on the Windows
command prompt, and in Powershell as well). But you could have followed along
using the user interface of your operating system. For example, in Chapter 11, I
list the contents of the `dev/` directory using the following command:

```
owner@localhost ➤ ls dev/
```

but you could have just opened the `dev/` folder in the file explorer of your
operating system of choice. But now, you will need to get to know Linux and the
Linux ecosystem and concepts. No worries, it’s not as difficult as it sounds,
and I think that you likely aren’t afraid of difficult things, or else you would
have stopped reading this book much earlier.

*Linux* is a not the name of any one specific operating system, but technically,
of an operating system kernel. A kernel is an important component of an
operating system. What sets the Linux kernel apart from the one used for Windows
or macOS, is that the Linux kernel is open source and free software. So anyone
can take that kernel, and add all the other components needed to build a
complete operating system. This is why there are many *Linux distributions*: a
Linux distribution is a complete operating system that use the Linux kernel. The
most popular Linux distribution is called Ubuntu, and if one time you googled
something along the lines of "easy linux os for beginners" the answer that came
out on top was likely Ubuntu, or one the other variants of Ubuntu (yes, because
Ubuntu itself is also open source and free software, it is possible to build a
variant using Ubuntu as a basis, like Linux Mint).

To define our Docker images, we will be using Ubuntu as a base. The Ubuntu
operating system has two releases a year, one in April and one in October. Every
two even years, the April release is long-term support (LTS) release. LTS
releases get security updates for 5 years, and Docker images generally use an
LTS release as a base. As of writing (April 2023), the current LTS is Ubuntu
22.04 *Jammy Jellyfish* (Ubuntu releases are named with a number of the form
YY.MM and then a code name based on some animal).

If you want, you can install Ubuntu on your computer. But there’s no need for
this, since you can use Docker to ship your projects.

A major difference between Ubuntu (and other Linux distributions) and macOS and
Windows, is how you install software. In short, software for Linux distributions
is distributed as packages. If you want to install, say, the Emacs text editor,
you can run the following command in the terminal:

```
sudo apt-get install emacs-gtk
```

Let’s break this down: `sudo` makes the next commands run as root. *root* is
Linux jargon for administrator. So if I type `sudo xyz`, the command `xyz` will
run with administrator privileges. Then comes `apt-get install`. `apt-get` is
Ubuntu’s package manager, and `install` is the command that install `emacs-gtk`.
`emacs-gtk` is the name of the Emacs package. Because you’re an R user, this
should be somewhat familiar: after all, extensions for R are also installed
using a package manager and a command: `install.packages("package_name")`. Just
like in R, where the packages get downloaded from CRAN, Ubuntu downloads
packages from a repository which you can browse
[here](https://packages.ubuntu.com/jammy/). Of course, because using the command
line is intimidating for beginners, it is also possible to install packages
using a software store, just like on macOS or Windows. But remember, Docker only
uses what it really needs to function, so there’s no interactive user interface.
This is not because Docker’s developers don’t like user interfaces, but because
the point of Docker is not to use Docker images interactively, so there’s no
need for the user interface. So you need to know how to install Ubuntu packages
with the command line.

Just like for R, it is possible to install software from different sources. It
is possible to add different repositories, and install software from there. We
are not going to use this here, but just as an aside, if you are using Ubuntu on
your computer as your daily driver operating system, you really should check out
[r2u](https://github.com/eddelbuettel/r2u), an Ubuntu repository that comes with
pre-compiled R packages that can get installed, very, very quickly. Even though
we will not be using this here (and I’ll explain why later in this chapter), you
should understand why r2u is so useful.

Let’s suppose that you are using Ubuntu on your machine, and are using R. If you want
to install the `{dplyr}` R package, something interesting happens when you type:

```{r, eval = F}
install.packages("dplyr")
```

On Windows and macOS, a compiled binary gets downloaded from CRAN and installed
on your computer. A "binary" is the compiled source code of the package. Many R
packages come with C++ or Fortran code, and this code cannot be used as is by R.
So these bits of C++ and Fortran code need to be compiled to be used. Think of
it this way: if the source code is the ingredients of a recipe, the compiled
binary is the cooked meal. Now imagine that each time you want to eat
Bouillabaisse, you have to cook it yourself... or you could get it delivered to
your home. You’d probably go for the delivery (especially if it would free)
instead of cooking it each time. But this supposes that there’s people out there
cooking Bouillabaisse for you. CRAN essentially cooks the package source codes
into binaries for Windows and macOS, as shown below:

<figure>
    <img src="images/tidyverse_packages.png"
         alt="Download links to pre-compiled tidyverse binaries."></img>
    <figcaption>Download links to pre-compiled tidyverse binaries.</figcaption>
</figure>

In the image above, you can see links to compiled binaries of the `{tidyverse}`
package for Windows and macOS, but none for any Linux distribution. This is
because, as stated in the introduction, there are many, many, many Linux
distributions. So at best, CRAN could offer binaries for Ubuntu, but Ubuntu is
not the only Linux distribution, and Ubuntu has two releases a year, which means
that every CRAN package (that needs compilation) would need to get compiled
twice a year. This is a huge undertaking, unless CRAN decided to only offer
binaries for LTS releases. But that would still be every two years.

So instead, what happens, is that the burden of compilation is pushed to the
user. Every time you type `install.packages("package_name")`, and if that
package requires compilation, that package gets compiled on your machine, which
not only takes some time, but can also fail. This is because very often, R
packages that require compilation need some additional system-level dependencies
that need to be installed. For example, here are the Ubuntu dependencies that need
to be installed for compilation of the `{tidyverse}` package to succeed:

```
libicu-dev
zlib1g-dev
make
libcurl4-openssl-dev
libssl-dev
libfontconfig1-dev
libfreetype6-dev
libfribidi-dev
libharfbuzz-dev
libjpeg-dev
libpng-dev
libtiff-dev
pandoc
libxml2-dev
```

This why r2u is so useful: by adding this repository, what you’re essentially
doing is telling R to not fetch the packages from CRAN, but from the r2u
repository. And this repository contains compiled R packages for Ubuntu. So the
required system-level dependencies get installed automatically and the R package
doesn’t need compilation. So installation of the `{tidyverse}` package takes
less than half a minute on a modern machine.

But if r2u is so nice, why did I say above that we would not be using it?
Unfortunately, this is because r2u does not archive compiled binaries of older
packages, and this is exactly what we need for reproducibility. So when you’re
building a Docker image to make a project reproducible, because that image will
be based on Ubuntu, we will need to make sure that our Docker image contains the
right system-level dependencies so that compilation of the R packages doesn’t
fail. Thankfully, you’re reading the right book.

## First steps with Docker

Let’s start by creating an "Hello World" Docker image. As I explained in the
beginning, to define a Docker image, we need to create a `Dockerfile` with some
instructions. But first, you need of course to install Docker. To install Docker
on any operating system (Windows, macOS or Ubuntu or other Linuxes), you can
install [Docker Desktop](https://docs.docker.com/desktop/). If you’re running
Ubuntu (or another Linux distribution) and don’t want the GUI, and are using
Ubuntu, you could install the [Docker
engine](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository)
and then follow the post-installation [steps for
Linux](https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user).

In any case, whatever operating system you’re using, we will be using the
command line to interact with Docker. Once you’re done with installing Docker,
create a folder somewhere on your computer, and create inside of this folder an
empty text file with the name `Dockerfile`. This can be tricky on Windows,
because you have to remove the `.txt` extension that gets added by default on
Windows. You might need to turn on the option "File name extensions" in the
`View` pane of the Windows file explorer to make this process easier.
Then, open this file with your favourite text editor, and add the following lines:

```
FROM ubuntu:jammy

RUN uname -a
```

This very simple `Dockerfile` does two things: it starts by stating that it’s
based on the Ubuntu Jammy (so version 22.04) operating system, and then runs the
`uname -a` command. This command, which gets run inside the Ubunu command line,
prints the Linux kernel version from that particular Ubuntu release. `FROM` and
`RUN` are Docker commands; there are a couple others that we will discover a bit
later. Now, what do you do with this `Dockerfile`? Remember, a `Dockerfile`
defines an image. So now, we need to build this image. Open a terminal/command
prompt in the folder where the `Dockerfile` is and type the following:

```
owner@localhost ➤ docker build -t raps_hello .
```

The `docker build` command builds an image from the `Dockerfile` that is in
path `.` (a single `.` means "this current working directory"). The `-t` option
tags that image with the name `raps_hello`. If everything goes well, you 
should see this output:

```
Sending build context to Docker daemon  2.048kB
Step 1/2 : FROM ubuntu:jammy
 ---> 08d22c0ceb15
Step 2/2 : RUN uname -a
 ---> Running in 697194b9a519
Linux 697194b9a519 6.2.6-1-default #1 SMP PREEMPT_DYNAMIC Mon Mar 13 10:57:27 UTC 2023 (fa1a4c6) x86_64 x86_64 x86_64 GNU/Linux
Removing intermediate container 697194b9a519
 ---> a0ea59f23d01
Successfully built a0ea59f23d01
Successfully tagged raps_hello:latest
```

Look at `Step 2/2`: you should see the output of the `uname -a` command:

```
Linux 697194b9a519 6.2.6-1-default #1 SMP PREEMPT_DYNAMIC Mon Mar 13 10:57:27 UTC 2023 (fa1a4c6) x86_64 x86_64 x86_64 GNU/Linux
```

Every `RUN` statement in the `Dockerfile` gets executed at build time: so this
is what we will use to install R and the needed packages. This way, once the 
image is built, we end up with an image that contains all the software we need.

Now, we would like to be able to use this image. Using a built image, we can
start one or several containers that we can use for whatever we want. Let's now
create a more realistic example. Let's build a Docker image that comes with R
pre-installed. But for this, we need to go back to our `Dockerfile` and change
it a bit:

```
FROM ubuntu:jammy

ENV TZ=Europe/Luxembourg

RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

RUN apt-get update && apt-get install -y r-base

CMD ["R"]
```

First we define a variable using `ENV`, called `TZ` and we set that to
the `Europe/Luxembourg` time zone (you can change this to your own time zone).
We then run a rather complex looking command that sets the defined time zone 
system-wide. We had to do all this, because when we will later install R,
a system-level dependency called `tzdata` gets installed. This tool then asks
the user, to enter his or her time zone interactively. But we cannot interact
with the image interactively as it's being built, so the build process gets
stuck at this prompt. By using these two commands, we can set the correct 
time zone and once `tzdata` gets installed, that tool doesn't ask for the time
zone anymore, so the build process can continue. This is a rather known issue
when building Docker images based on Ubuntu, so the fix is easily found with 
a Google search (but I'm giving it to you, dear reader, for free).

Then come `RUN` statements. The first one uses Ubuntu's package manager to first
refresh the repositories (this ensures that our local Ubuntu installation
repositories are in synch with the latest software updates that were pushed to
the central Ubuntu repos). Then we use Ubuntu's package manager to install
`r-base`. `r-base` is the package that installs R. We then finish this
`Dockerfile` by running `CMD ["R"]`. This is the command that will be executed
when we run the container.

Let's build the image (this will take some time, because a lot of software gets
installed):

```
owner@localhost ➤ docker build -t raps_ubuntu_r .
```

This builds an image called `raps_ubuntu_r`. This image is based on Ubuntu 
22.04 Jammy Jellyfish and comes with R pre-installed. But the version of R
that gets installed is the one made available through the Ubuntu repositories,
and as of writing that is version 4.1.2, while the latest version available is
R version 4.2.3. So the version available through the Ubuntu repositories lags
behind the actual release. But no matter, we will deal with that later.

We can now start a container with the following command:

```
owner@localhost ➤ docker run raps_ubuntu_r
```

And this is the output we get:

```
Fatal error: you must specify '--save', '--no-save' or '--vanilla'
```

What is going on here? When you run a container, the command specified by `CMD`
gets executed, and then the container quits. So here, the container ran the
command `R`, which started the R interpreter, but then quit immediately. When
quitting R, users should specify if they want to save or not save the workspace.
This is what the message above is telling us. So, how can be use this? Is there
a way to use this R version interactively?

Yes, there is a way to use this R version boxed inside our Docker image
interactively, even though that's not really what we want to achieve. What we
want instead is that our pipeline gets executed when we run the container. We
don't want to mess with the container interactively. But let me show you how
we can interact with this dockerized R version. First, you need to let the container
run in the background. You can achieve this by running the following command:

```
owner@localhost ➤ docker run -d -it --name ubuntu_r_1 raps_ubuntu_r
```

This runs the container that we name "ubuntu_r_1" from the image "raps_ubuntu_r"
(remember that we can run many containers from one single image definition).
Thanks to the option `-d`, the container runs in the background, and the 
option `-it` states that we want an interactive shell running. So the container
runs in the background, with an interactive shell waiting for us, instead
of launching (and then immediately stopping) the R command. You can now 
"connect" to the interactive shell and start R in it using:

```
owner@localhost ➤ docker exec -it ubuntu_r_1 R
```

You should now see the familiar R prompt:

```
R version 4.1.2 (2021-11-01) -- "Bird Hippie"
Copyright (C) 2021 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
```

Welcome to a dockerized version of R. Now, all of this might have felt overly
complicated to you. And of course if this is the first time that you have
played around with Docker, it is tricky indeed. However, you shouldn't worry
too much about it, for several reasons:

- we are not going to use Docker interactively, that's not really the point.
- we will build our images on top of pre-built images from the [Rocker project](https://rocker-project.org/) and these images come with a lot of software pre-installed and configuration taken care of. 

What you should take away from this section is that you need to write a 
`Dockerfile` which then allows you to build an image. This image can then
be used to run one (or several) containers. These containers, at run time,
will execute our pipeline in an environment that is frozen, such that the
output of this run will stay constant, forever.

## The Rocker project

The Rocker project offers a very large collection of "R-ready" Docker images
that you can use for you own projects. Before using these images though, I still
need to explain one very important Docker concept. Let's go back to our "Hello
World" Docker image:

```
FROM ubuntu:jammy

RUN uname -a
```

The very first line, `FROM ubuntu:jammy` downloads an Ubuntu Jammy image, but
from where? All these images get download from *Docker Hub*, which you can
browse [here](https://hub.docker.com/)^[https://hub.docker.com/]. If you create
an account you can even push your own images on there. For example we could push
the image we built before, which we called `raps_ubuntu_r`, on Docker Hub. Then,
if we wanted to create a new Docker image that builds upon `raps_ubuntu_r` we
could simply type `FROM username:raps_ubuntu_r` (or something similar).

It's also possible to not use Docker Hub at all, and share the image you
built as a file. We will explore this option later.

The Rocker project offers many different images, which are described
[here](https://rocker-project.org/images/)^[https://rocker-project.org/images/].
We are going to be using the *versioned stack*. These are images that ship
specific versions of R, which get built from source. This way, it doesn't matter
when the image gets build, the same version of R will be installed. Let me
explain why building R from source is important. When we build the image from
the Dockerfile we wrote before, R gets installed from the Ubuntu repositories.
For this we use Ubuntu's package manager and the following command: `apt-get
install -y r-base`. As of writing, the version of R that gets installed is
version 4.1.3. There's two problems with installing R from Ubuntu's
repositories. First, we have to use whatever gets installed, which can be a
problem with reproducibility. If we ran our analysis using R version 4.2.1, then
we would like to dockerize that version of R. The second problem is that when we
build the image today we get version 4.1.3. But it is not impossible that if we
build the image in 6 months, we get R version 4.2.0, because it is likely that
the version that ships in Ubuntu's repositories will get updated at some point.

This means that depending on *when* we build the image, we might get a different
version of R. There are only two ways of avoiding this problem: either we build
the image once, and archive it and make sure to always keep a copy and ship that
copy forever (or for as long as we want to make sure that project is
reproducible) just as you would ship data, code and any documentation required
to make the project reproducible. Or we write the Dockerfile in such a way that
it always produces the same image, regardless of *when* it gets built. I very
strongly advise you to go for the second option, but to *also* archive the
image. But of course, this also depends on how critical your project is. Maybe
you don't need to start archiving images, or maybe you don't even need to make
sure that the Dockerfile always produces the same image. But I would still
highly recommend that you write your Dockerfiles in such a way that they always
output the same image. It is safer, and it doesn't really mean extra work,
thanks to the Rocker project.

So, let's go back to the Rocker project, and specifically their *versioned*
images which you can find
[here](https://rocker-project.org/images/versioned/r-ver.html)^[https://rocker-project.org/images/versioned/r-ver.html]. When you use one the versioned
images as a base for your project, you get the following guarantees:

- a fixed version of R that gets built from source. It doesn't matter *when* you build the image, it will always ship with the same version of R;
- the operating system will be the LTS release that was current when that specific version of R was current;
- the R repositories are set to the Posit Public Package Manager (PPPM) at a specific date. This ensures that R packages don't need to be compiled as PPPM serves binary packages for the amd64 architecture (which is the architecture that virtually all non-Apple computers use these days).

For installing the right version of R packages, we don't need to worry too much
about the repositories. We will be using the `renv.lock` file to ensure that the
right packages get installed. As a quick introduction to using Rocker images,
let's grab the our project's `renv.lock` file which you can download from
[here](https://raw.githubusercontent.com/rap4all/housing/renv/renv.lock)^[https://is.gd/WB3KfN].

The important piece of information the `renv.lock` file gives us, is the R
version that we need to use, in this case, R version 4.2.2. So that's the
version we will be using in our Dockerfile. Next, we need to check the version
of `{renv}` that we used to build the `renv.lock` file. You don't necessarily
need to install the same version, but I recommend you do. For example, as I'm
writing these lines, `{renv}` version 0.17.1 is available, but the `renv.lock`
file was written by `{renv}` version 0.16.0. So to avoid any compatibility
issue, we will also install the exact same version. Thankfully, that is quite
easy to do (to check the version of `{renv}` that was used to write the lock
file simply look for the word "renv" in the lock file).

Create a new folder and call whatever you want and save the `renv.lock` file
linked above inside of it. Then, create an empty text file and call it
`Dockerfile`. Add the following lines:

```
FROM rocker/r-ver:4.2.2

RUN R -e "install.packages('remotes')"

RUN R -e "remotes::install_github('rstudio/renv@0.16.0')"

RUN mkdir /home/housing

COPY renv.lock /home/housing/renv.lock

RUN R -e "setwd('/home/housing');renv::restore()"
```

The first line states that we will be basing our image on the image
from the Rocker project that ships with R version 4.2.2, which is the
right version that we need. Then, we install the `{remotes}` package. This
will allow us to download a specific version from `{renv}` from Github.

## Dockerizing our project

## Sharing your images

### Sharing on Docker Hub

### Sharing a compressed archive of your image

This doesn't compress the image:

```
owner@localhost ➤ docker save raps_ubuntu_r > raps_ubuntu_r.tar
```

you can download it like so:

```
owner@localhost ➤ docker load < raps_ubuntu_r.tar
```

but if you want to compress it, you can run the following (but this might
only work on a Linux distribution, and only if the `xz` program is installed):

```
owner@localhost ➤ docker save raps_ubuntu_r | xz > raps_ubuntu_r.tar.xz
```
